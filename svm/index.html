
 <!DOCTYPE HTML>
<html lang="default">
<head>
  <meta charset="UTF-8">
  
    <title>支持向量机SVM | Stackess</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="cstackess">
    

    
    <meta name="description" content="支持向量机SVM学习笔记整理。">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机SVM">
<meta property="og:url" content="http://www.stackess.com/svm/index.html">
<meta property="og:site_name" content="Stackess">
<meta property="og:description" content="支持向量机SVM学习笔记整理。">
<meta property="og:image" content="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/613677762.gif">
<meta property="og:image" content="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/1048240534.png">
<meta property="og:image" content="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/2067743219.png">
<meta property="og:image" content="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/2820125685.png">
<meta property="og:image" content="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/3296738673.png">
<meta property="og:updated_time" content="2016-04-08T16:56:29.757Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="支持向量机SVM">
<meta name="twitter:description" content="支持向量机SVM学习笔记整理。">
<meta name="twitter:image" content="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/613677762.gif">

    
    <link rel="alternative" href="/atom.xml" title="Stackess" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/Blue_sawg.ico">
    
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Stackess">Stackess</a></h1>
				<h2 class="blog-motto">在互联网世界做只小透明</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:www.stackess.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/svm/" title="支持向量机SVM" itemprop="url">支持向量机SVM</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="cstackess" target="_blank" itemprop="author">cstackess</a>
		
  <p class="article-time">
    <time datetime="2015-04-14T06:38:00.000Z" itemprop="datePublished"> Published 2015-04-14</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-线性分类"><span class="toc-number">1.</span> <span class="toc-text">1 线性分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-magrin"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 magrin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-优化问题"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 优化问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Lagrangian对偶"><span class="toc-number">2.</span> <span class="toc-text">2 Lagrangian对偶</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-非线性扩展"><span class="toc-number">3.</span> <span class="toc-text">3 非线性扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-映射"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-核函数"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-几个核函数"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 几个核函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-异常点"><span class="toc-number">4.</span> <span class="toc-text">4 异常点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-主要参考"><span class="toc-number">5.</span> <span class="toc-text">5 主要参考</span></a></li></ol>
		
		</div>
		
		<p>支持向量机SVM学习笔记整理。<br><img src="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/613677762.gif" alt="overview.gif"><br><a id="more"></a></p>
<h2 id="1-线性分类"><a href="#1-线性分类" class="headerlink" title="1 线性分类"></a>1 线性分类</h2><p>先假设线性可分的情形。这里我们考虑的是一个两类的分类问题。</p>
<ul>
<li>数据点：$x_1,x_2,\ldots,x_n$ 共 $n$ 个点，每个点表示为m维向量 $x=(x^{(1)},x^{(2)},\ldots,x^{(m)})^T$ ，</li>
<li>线性分类器：在 $R^m$ 中找到一个超平面（hyperplane）将数据点分为两类</li>
<li>类别：表示为 $y$，取 +1 或者 -1 </li>
</ul>
<p>$R^m$ 的<code>超平面</code>表示为<br>$$<br>\begin{equation} \langle  w,x \rangle +b=w^Tx+b=0\end{equation}<br>$$<br>其中向量 $w=(w^{(1)},w^{(2)},\ldots,w^{(m)})^T$，$\langlew,x\rangle$ 为向量内积，$w^Tx$ 则是从矩阵角度看，且有 $w^Tw=|w|^2$ 。</p>
<blockquote>
<p>回忆起高中的数学知识，二维空间中这样的超平面就是任一直线，表示为$Ax+By+C=0$。因此 $m$ 维空间中的超平面为 $w^Tx+b=w^{(1)}x^{(1)}+w^{(2)}x^{(2)}+\ldots+w^{(m)}x^{(m)}+b=0$。</p>
</blockquote>
<p><code>分类函数</code>定义为<br>$$\begin{equation}f(x)=w^Tx+b\end{equation}$$<br>则在超平面上的 $x$ 有 $f(x)=0$，其余则 $f(x)&gt;0$ 或 $f(x)&lt;0$。</p>
<p>线性分类器对超平面的挑选标准是令数据点和超平面间的margin越大越好，这样confidence就越高。 确定超平面后，对一个数据点 $x$ 进行分类，实际上是通过把 $x$ 带入到分类函数 $f(x)=w^Tx+b$ 算出结果然后根据其正负号来进行类别划分的。</p>
<h3 id="1-1-magrin"><a href="#1-1-magrin" class="headerlink" title="1.1 magrin"></a>1.1 magrin</h3><p>需要考虑两种 magrin。</p>
<p><code>functional margin</code><br>$$\begin{equation}\hat{\gamma}=yf(x)=y(w^Tx+b)\end{equation}$$</p>
<blockquote>
<p>从刚才 $f(x)=0$，$f(x)&gt;0$ 和 $f(x)&lt;=0$ 的三种情况，容易想到 $f(x)$ 的值可以衡量 $x$ 到超平面的margin。并乘上对应的类别 $y$ 值（+1或-1）来消除正负号的影响，使得functional margin $\hat{\gamma}$ 恒为正。</p>
</blockquote>
<p> 但是，通过等比例地缩放 $w$ 和 $b$ 的值，可使得超平面 $w^Tx+b=0$ 不变，但 $f(x)=w^Tx+b$ 的值任意大，令实际距离不变的数据点和超平面间的 functional margin $\hat{\gamma}$ 也任意大，岂不是这样就最大化 functional margin了？因此以 functional margin 作为衡量标准并不合理。</p>
<p><code>geometrical margin</code><br>$$\begin{equation}\tilde{\gamma} = \frac{yf(x)}{|w|}= \frac{\hat{\gamma}}{|w|}\end{equation}$$</p>
<blockquote>
<p> <strong>证明：</strong><br> 对于一个点 $x$ ，令其垂直投影到超平面上的对应的点为 $x_0$，$f(x_0)=0$，$w$ 是垂直于超平面的一个向量（因为设超平面上任意两点 $x_1$ 和 $x_2$，则 $w^Tx_1+b=0$，$w^Tx_2+b=0$，两式相减得 $w^T(x_1-x_2)=0$，即 $w$ 向量和超平面互相垂直），$\tilde{\gamma}$ 为 $x$ 到超平面的几何距离，依然用 $y$ 来保持非负性。因此有<br> $$\tilde{\gamma}\frac{w}{|w|}=y(x-x_0)$$<br> 整理得 $x=x_0+\frac{\tilde{\gamma}}{y}\frac{w}{|w|}$，代入 $f(x)$ 得<br> $$f(x)=f(x_0+\frac{\tilde{\gamma}}{y}\frac{w}{|w|})=w^T(x_0+\frac{\tilde{\gamma}}{y}\frac{w}{|w|})+b=(w^Tx_0+b)+\frac{\tilde{\gamma}}{y}\frac{w^Tw}{|w|}=\frac{\tilde{\gamma}}{y}|w|$$<br> 即$$\tilde{\gamma}=\frac{yf(x)}{|w|}$$<br> <img src="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/1048240534.png" alt="1428654312514.png"></p>
<p>高中数学又立功了！二维空间中点 $P(x_0,y_0)$ 到直线 $Ax+By+C=0$ 的距离公式为 $d=\frac{|Ax_0+By_0+C|}{\sqrt{A^2+B^2}}$。</p>
</blockquote>
<p>显然，functional margin 和 geometrical margin 相差一个 $|w|$ 的缩放因子。由于 geometrical margin 多了 $|w|$ 这个分母，所以同比例缩放 $w$ 和 $b$ 的时候，超平面不会变， 数据点和超平面间的 geometrical margin $\tilde{\gamma}=\frac{y(w^Tx+b)}{|w|}$ 的值也不会变。因此，最大化 geometrical margin 才是我们真正想要的。</p>
<p>那废话这么多，为什么要介绍functional margin？因为它可以简化优化问题！</p>
<h3 id="1-2-优化问题"><a href="#1-2-优化问题" class="headerlink" title="1.2 优化问题"></a>1.2 优化问题</h3><p>实际上，在超平面（以下开始称为<code>分割面</code>）两边存在着gap，它的边界是另外两个平行的超平面，为了区别，我称它俩为<code>支持面</code>。两个支持面到分割面的距离相等，这个距离就是所能得到的最大的 geometrical margin $\tilde{\gamma}$ ，它是gap的一半。而必然有一些点“支持”这两个支持面（否则就可以进一步地扩充 gap ，这就不是最大的 margin 了），我称这些点为<code>支持点</code>。点可以表示为以原点为起始点的向量，因此也叫做 <code>support vector</code>。</p>
<p><img src="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/2067743219.png" alt="1428670521444.png"></p>
<p>最大间隔分类器（maximum margin classifier）的目标函数可以定义为：<br>$$<br>\max\tilde{\gamma}=\max\frac{\hat{\gamma}}{|w|} \<br>s.t.\hat{\gamma_i}\geq \hat{\gamma},\quad i=1,2,\ldots,n\<br>$$<br>其中不等式约束 $\hat{\gamma_i}\geq \hat{\gamma}$ 来自于 $\hat{\gamma}=\min \hat{\gamma}_i $，指的是 $n$ 个数据点到分割面的 functional margin 要大于支持面到分割面的 functional margin，因为显然所有点都在支持面“身后”。</p>
<p>这个优化问题还可以进一步简化。前面说过，可以通过等比例地缩放 $w$ 和 $b$ 的值，使得超平面不变，而 functional margin $\hat{\gamma}$ 任意变化。因此可以为所有的 $\hat{\gamma_i}$ 挑一个基准 $w$ 和 $b$ ，刚好令 $\hat{\gamma}=\min \hat{\gamma}_i=y(w^Tx+b)=1$，问题就简化为：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\max \frac{1}{|w|} \\<br>s.t.  &amp;y_i(w^Tx_i+b)\geq 1,\quad i=1,2,\ldots,n\\<br>\end{array}\end{equation}$$</p>
<p> 而求 $\max \frac{1}{|w|}$ 也就是求 $\min |w|$。其实后面会有对 $w$ 的求导，且有 $w^Tw=|w|^2$， 因此等价求 $\min \frac{1}{2}|w|^2$ 实际上为 Lagrangian 对偶做好了铺垫。</p>
<p>最后优化问题就简化为：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\min \frac{1}{2}|w|^2 \\<br>s.t.  &amp;y_i(w^Tx_i+b)-1\geq 0 ,\quad i=1,2,\ldots,n\\<br>\end{array}\end{equation}$$</p>
<p>因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的QP (Quadratic Programming) 优化包进行求解。一言以蔽之：在一定的约束条件下，目标最优，损失最小。解完该优化问题，就可以获得最优的 $w^∗$，$b^∗$，确定分类函数 $f(x)={w^∗}^Tx+b$。</p>
<blockquote>
<p><code>凸规划</code>：求凸函数在凸集上的极小点。</p>
<ul>
<li>凸规划的局部极小点就是全局极小点，且极小点的集合是凸集。</li>
<li>如果凸规划的目标函数是严格凸函数，又存在极小点，那么它的极小点是唯一的。</li>
</ul>
<p><code>二次规划</code>：是非线性规划中一种特殊情况，目标函数是二次实函数，约束是线性的。</p>
<p>提供了解决<a href="http://en.wikipedia.org/wiki/Quadratic\_programming" target="_blank" rel="external">二次规划</a>问题的包的包括Excel、Matlab、R、SAS等等。</p>
</blockquote>
<hr>
<h2 id="2-Lagrangian对偶"><a href="#2-Lagrangian对偶" class="headerlink" title="2 Lagrangian对偶"></a>2 Lagrangian对偶</h2><p>注意到目标函数的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与 primal problem（原问题）等价的 dual problem （对偶问题）得到 primal problem 的最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。</p>
<p>Lagrangian如下：<br>$$\begin{equation}<br>\mathcal{L}(w,b,\alpha)=\frac{1}{2}|w|^2-\sum_{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1\right)<br>\end{equation}$$<br>则<br>$$ \begin{equation}<br>\min_{w,b} \max_{\alpha_i\geq 0} \mathcal{L}(w,b,\alpha)\\ \text{等价于}\min_{y_i(w^Tx_i+b)-1\geq 0} \frac{1}{2}|w|^2<br>\end{equation}$$<br>它们被称为<code>primal problem</code>。</p>
<blockquote>
<p><strong>证明：</strong><br>对内层的优化问题 $z(w)=$<br>\begin{array}{*{20}{l}}<br>&amp;\max \mathcal{L}(w,b,\alpha) \\<br>s.t.  &amp;\alpha_i\geq 0 ,\quad i=1,2,\ldots,n\\<br>\end{array}<br>可分为两种情况讨论：</p>
<ul>
<li>当所有数据点都满足 $y_i(w^Tx_i+b)-1\geq 0$ 的约束时，支持点始终取到 $0$，非支持点大于 $0$，而要达到 $\mathcal{L}(w,b,\alpha)$ 的最大值，则非支持点对应的 $\alpha_i=0$，从而达到最大值，即 $\frac{1}{2}|w|^2$。</li>
<li>当存在数据点 $y_i(w^Tx_i+b)-1\leq 0$ 时，$\mathcal{L}(w,b,\alpha)$ 的最大值可通过 $\alpha_i\rightarrow+\infty$ 趋近于无穷大。</li>
</ul>
<p>因此求 $\min_{w,b}z(w)$ 时，满足约束的第一种情况就等价于求 $\min_{y_i(w^Tx_i+b)-1\geq 0} \frac{1}{2}|w|^2$，而不满足约束的第二种情况，对无穷大求最小值，自然不会是所要的。</p>
</blockquote>
<p>与之对应的是<code>daul problem</code>：<br>$$\begin{equation}\max_{\alpha_i\geq 0} \min_{w,b} \mathcal{L}(w,b,\alpha)\end{equation}$$</p>
<p>记 primal problem 的最小值记为 $p^∗$ ，dual problem 的最大值为 $d^∗$，则<strong>对于所有的优化问题</strong>都有<code>weak duality</code>性质：<br>$$\begin{equation}d^∗\leq p^∗\end{equation}$$<br>而当<strong>某些特殊条件</strong>满足时，有<code>strong duality</code>性质：<br>$$\begin{equation}d^∗= p^∗\end{equation}$$<br>比如 凸规划+<code>KKT条件</code> 或 凸规划+<code>Slater条件</code>。</p>
<p>primal problem 满足，因此 strong duality 性质成立。优化问题由 primal problem 转换为求解 daul problem $\max_{\alpha_i\geq 0} \min_{w,b} \mathcal{L}(w,b,\alpha)$。</p>
<p>对内层 $\min_{w,b} \mathcal{L}(w,b,\alpha)$ 的求解可通过：<br>$$\begin{align}<br>\frac{\partial \mathcal{L}}{\partial w}=0 &amp;\Rightarrow w=\sum_{i=1}^n \alpha_i y_i x_i \\<br>\frac{\partial \mathcal{L}}{\partial b} = 0 &amp;\Rightarrow \sum_{i=1}^n \alpha_i y_i = 0<br>\end{align}$$<br>代回得<br>$$\begin{equation}<br>\min_{w,b} \mathcal{L}(w,b,\alpha)=\sum_{i=1}^n \alpha_i-\frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_iy_j x_i^Tx_j<br>\end{equation}$$</p>
<blockquote>
<p><strong>证明：</strong><br>$$\begin{array}{*{20}{lr}}<br>\frac{1}{2}|w|^2-\sum_{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1\right)&amp;\\<br>=\frac{1}{2}w^Tw-\sum_{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1\right)<br>&amp;(w^Tw=|w|^2)\\<br>=\frac{1}{2}w^T\sum_{i=1}^n \alpha_i y_i x_i-w^T\sum_{i=1}^n \alpha_i y_i x_i-\sum_{i=1}^n \alpha_i y_ib+\sum_{i=1}^n \alpha_i<br>&amp;(代入w)\\<br>=\sum_{i=1}^n \alpha_i-\frac{1}{2}w^T\sum_{i=1}^n \alpha_i y_i x_i<br>&amp;(\sum_{i=1}^n \alpha_i y_i = 0 )\\<br>=\sum_{i=1}^n \alpha_i-\frac{1}{2}\left(\sum_{i=1}^n \alpha_i y_i x_i\right)^T\sum_{i=1}^n \alpha_i y_i x_i<br>&amp;(代入w^T)\\<br>=\sum_{i=1}^n \alpha_i-\frac{1}{2}\sum_{i=1}^n \alpha_i y_i x_i^T\sum_{i=1}^n \alpha_i y_i x_i<br>&amp;\\<br>=\sum_{i=1}^n \alpha_i-\frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_iy_j x_i^Tx_j<br>&amp;(\sum_i a_i\sum_j b_j=\sum_{i,j} a_ib_j)<br>\end{array} $$</p>
</blockquote>
<p>即 $\max_{\alpha_i\geq 0} \min_{w,b} \mathcal{L}(w,b,\alpha)$ 简化为：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\max \sum_{i=1}^n\alpha_i – \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j  \\<br>s.t., &amp;\alpha_i\geq 0, i=1,\ldots,n \\<br>&amp;\sum_{i=1}^n\alpha_iy_i = 0<br>\end{array}\end{equation}$$</p>
<p>也将 $w=\sum_{i=1}^n \alpha_i y_i x_i$ 代入分类函数中<br>$$\begin{equation}<br>f(x)= w^Tx+b=\left(\sum_{i=1}^n\alpha_i y_i x_i\right)^Tx+b<br> = \sum_{i=1}^n\alpha_i y_i  x_i^T x + b<br>\end{equation}$$</p>
<p>最后，用<code>向量内积</code>形式 $\langle x_i, x\rangle$ 替换 $x_i^Tx$。<br>优化问题：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\max \sum_{i=1}^n\alpha_i – \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_j \langle x_i, x_j\rangle   \\<br>s.t., &amp;\alpha_i\geq 0, i=1,\ldots,n \\<br>&amp;\sum_{i=1}^n\alpha_iy_i = 0<br>\end{array}\end{equation}$$<br>分类函数：<br>$$\begin{equation}<br>f(x)=\sum_{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>\end{equation}$$</p>
<p>转换为内积带来以下好处：</p>
<ul>
<li>对于新点 $x$ 的预测，只需要计算它与训练数据点的内积即可。</li>
<li>内积可以使用 Kernel 函数进行非线性推广。</li>
<li>非支持点对应的 $\alpha_i=0$ ，则新点的内积计算实际上只要针对少量的支持点，而不是所有的训练数据点。这就体现了 “Supporting Vector”。</li>
</ul>
<p>通过解优化问题（如 SMO 算法），得到 $\alpha_i^∗$，代入得 $w^∗=\sum_{i=1}^n \alpha_i^∗ y_i x_i$，$b^∗=-\frac{\max_{y_i=-1}{w^∗}^Tx_i+\min_{y_i=1}{w^∗}^Tx_i}{2}$。</p>
<hr>
<h2 id="3-非线性扩展"><a href="#3-非线性扩展" class="headerlink" title="3 非线性扩展"></a>3 非线性扩展</h2><h3 id="3-1-映射"><a href="#3-1-映射" class="headerlink" title="3.1 映射"></a>3.1 映射</h3><p>前面介绍了线性情况下的支持向量机，它通过寻找一个线性的超平面来达到对数据进行分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。</p>
<p><img src="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/2820125685.png" alt="1428848645116.png"></p>
<p>通过映射 $\phi(\cdot)$ 将分类函数 $f(x)=\sum_{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b $ 向高维空间映射，可使得其线性可分：<br>$$\begin{equation} f(x) = \sum_{i=1}^n\alpha_i y_i \color{red}{\langle \phi(x_i), \phi(x)\rangle} + b\end{equation}$$</p>
<p>其中的 $\alpha_i$通过求解 dual problem 而得：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\max_\alpha \sum_{i=1}^n\alpha_i – \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_j\color{red}{\langle \phi(x_i),\phi(x_j)\rangle} \\<br>s.t., &amp;\alpha_i\geq 0, i=1,\ldots,n \\<br>&amp;\sum_{i=1}^n\alpha_iy_i = 0<br>\end{array}\end{equation}$$</p>
<blockquote>
<p><strong>例1：</strong><br>$R^2$ 的点表示为 $x(\eta_1,\eta_2)^T$ ，则二次曲线可以写作：<br>$$a_1\eta_1 + a_2\eta_1^2 + a_3 \eta_2 + a_4\eta_2^2 + a_5\eta_1 \eta_2 + a_6 = 0$$<br>通过坐标映射 $\phi:(\eta_1,\eta_2)^T\rightarrow(z_1,z_2,z_3,z_4,z_5)^T$ 到 $R^5$ 中，对应点 $\phi(x)$ 的 $z_1=\eta_1$，$z_2=\eta_1^2$，$z_3=\eta_2$，$z_4=\eta_2^2$，$z_5=\eta_1 \eta_2$。那么显然，上面的方程在新的坐标系下可以写作：<br>$$\sum_{i=1}^5a_i z_i + a_6 = 0$$<br>从新的坐标 $z_i$ 角度看，这是一个超平面方程。因此， $R^2$ 中需要二次曲线分割（线性不可分）的数据点在 $R^5$ 中将可用超平面分割（线性可分）。</p>
<p>高中数学示范， $P(x,y)$ 是圆 $x^2+(y-1)^2=4$ 上的任意一点，即满足 $x^2-2y+y^2-3=0$，通过将原来的$f(x,y)$的二次函数看作 $g(x^2,y,y^2)$ 的线性函数，就可以映射到三维空间中对应的点 $P(x^2,y,y^2)$。</p>
</blockquote>
<p>直接向高维映射的方法看似成功，但是问题是：</p>
<ul>
<li>映射维数爆炸：在最初的例子里，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；如果原始空间是三维，那么我们会得到 19 维的新空间（验算一下？），这个数目是呈爆炸性增长的，这给 $\phi(\cdot)$ 的计算带来了非常大的困难。</li>
<li>无穷维情况：无从计算。</li>
</ul>
<h3 id="3-2-核函数"><a href="#3-2-核函数" class="headerlink" title="3.2 核函数"></a>3.2 核函数</h3><p>所以就需要 <code>Kernel</code> 出马了。核函数能简化映射空间中的内积运算，避开了直接在高维空间中进行计算，而结果却是等价的。</p>
<p><img src="http://7vzozf.com1.z0.glb.clouddn.com/2015/04/3296738673.png" alt="1428895104000.png"></p>
<p>用 $\kappa(x_i,x)$ 的低维核函数代替 $\langle \phi(x_i), \phi(x)\rangle$ 的高维内积运算，使得分类函数为：</p>
<p>$$f(x)=\sum_{i=1}^n\alpha_i y_i \color{red}{\kappa(x_i,x)} + b$$</p>
<p>其中的 $\alpha_i$通过求解 dual problem 而得：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\max_\alpha\sum_{i=1}^n\alpha_i – \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_j\color{red}{\kappa(x_i,x_j)} \\<br>s.t., &amp;\alpha_i\geq 0, i=1,\ldots,n \\<br>&amp;\sum_{i=1}^n\alpha_iy_i = 0<br>\end{array}\end{equation}$$</p>
<blockquote>
<p><strong>例2：</strong><br>设 $R^2$ 有两点 $x_1 = (\eta_1,\eta_2)^T$ 和 $x_2=(\xi_1,\xi_2)^T$，通过例1的映射 $\phi(\cdot)$ 得到 $R^5$ 中的对应点 $\phi(x_1)(\eta_1,\eta_1^2,\eta_2,\eta_2^2,\eta_1\eta_2)^T$，$\phi(x_2)(\xi_1,\xi_1^2,\xi_2,\xi_2^2,\xi_1\xi_2)^T$，映射内积为<br>$$\langle \phi(x_1),\phi(x_2)\rangle = \eta_1\xi_1 + \eta_1^2\xi_1^2 + \eta_2\xi_2 + \eta_2^2\xi_2^2+\eta_1\eta_2\xi_1\xi_2$$<br>令核函数 $\kappa(x_1,x_2)=\left(\langle x_1, x_2\rangle + 1\right)^2$， 则<br>$$\kappa(x_1,x_2)= 2\eta_1\xi_1 + \eta_1^2\xi_1^2 + 2\eta_2\xi_2 + \eta_2^2\xi_2^2 + 2\eta_1\eta_2\xi_1\xi_2 + 1$$<br>与 $\langle \phi’(x_1),\phi’(x_2)\rangle $ 结果相等，其中$\phi’(x_1)(\sqrt{2}\eta_1,\eta_1^2,\sqrt{2}\eta_2,\eta_2^2,\sqrt{2}\eta_1\eta_2,1)$，$\phi’(x_2)(\sqrt{2}\xi_1,\xi_1^2,\sqrt{2}\xi_2,\xi_2^2,\sqrt{2}\xi_1\xi_2,1)$。显然 $\phi’(\cdot)$也就是把 $\phi(\cdot)$ 某几个维度线性缩放一下，然后再加上一个常数维度。</p>
<p>当然，因为我们这里的例子非常简单，所以我可以手工构造出 $\phi(\cdot)$ 对应的核函数出来，如果对于任意一个映射，想要构造出对应的核函数就很困难了。</p>
</blockquote>
<p>原先需要映射到高维空间中，然后再根据内积的公式进行计算；现在可以直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。这样就可以解决映射维度爆炸的问题，甚至是无穷维度的情况也没有问题。 </p>
<h3 id="3-3-几个核函数"><a href="#3-3-几个核函数" class="headerlink" title="3.3 几个核函数"></a>3.3 几个核函数</h3><p>通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：</p>
<ul>
<li>多项式核 $\kappa(x_1,x_2) = \left(\langle x_1,x_2\rangle + R\right)^d$ </li>
<li>高斯核 $\kappa(x_1,x_2) = \exp\left(-\frac{|x_1-x_2|^2}{2\sigma^2}\right)$</li>
<li>线性核 $\kappa(x_1,x_2) = \langle x_1,x_2\rangle$</li>
</ul>
<p>除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</p>
<hr>
<h2 id="4-异常点"><a href="#4-异常点" class="headerlink" title="4 异常点"></a>4 异常点</h2><p>SVM 允许数据点在一定程度上偏离一下超平面。</p>
<p>修改原来的约束条件 $y_i(w^Tx_i+b)\geq 1$ 为：<br>$$y_i(w^Tx_i+b)\geq 1\color{red}{-\xi_i}, \quad i=1,\ldots,n$$</p>
<p>其中 $\xi_i\geq0$ 称为<code>松弛变量</code> (slack variable) ，对应数据点 $x_i$ 允许偏离的 functional margin 的量。当然，如果我们运行  $\xi_i\geq0$  任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些  $\xi_i\geq0$  的总和也要最小，即<br>$$\min \frac{1}{2}|w|^2\color{red}{+C\sum_{i=1}^n \xi_i}$$<br>其中 $C$ 是一个参数，用于控制目标函数中两项（“寻找 margin 最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中 $\xi_i$ 是需要优化的变量（之一），而 $C$ 是一个事先确定好的常量。</p>
<p>优化问题：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\min\frac{1}{2}|w|^2 + C\sum_{i=1}^n\xi_i \\<br>s.t., &amp; y_i(w^Tx_i+b)\geq 1-\xi_i, i=1,\ldots,n \\<br>&amp; \xi_i \geq 0, i=1,\ldots,n<br>\end{array}\end{equation}$$</p>
<p> Lagrangian：<br>$$\mathcal{L}(w,b,\xi,\alpha,r)=\frac{1}{2}|w|^2 + C\sum_{i=1}^n\xi_i – \sum_{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1+\xi_i\right) – \sum_{i=1}^n r_i\xi_i$$</p>
<p>解 $\min_{w,b,\xi}\mathcal{L}(w,b,\xi,\alpha,r)$：<br>$$\begin{align}<br>\frac{\partial \mathcal{L}}{\partial w}=0 &amp;\Rightarrow w=\sum_{i=1}^n \alpha_i y_i x_i \\<br>\frac{\partial \mathcal{L}}{\partial b} = 0 &amp;\Rightarrow \sum_{i=1}^n \alpha_i y_i = 0 \\<br>\frac{\partial \mathcal{L}}{\partial \xi_i} = 0 &amp;\Rightarrow C-\alpha_i-r_i=0, \quad i=1,\ldots,n<br>\end{align}$$<br>注意到 $C−\alpha_i−r_i=0$ ，又有 $r_i\geq 0$ （作为 Lagrange multiplier 的条件），因此有 $\alpha_i\leq C$ 。</p>
<p>代回依然得<br>$$\begin{equation}<br>\min_{w,b,\xi}\mathcal{L}(w,b,\xi,\alpha,r)=\sum_{i=1}^n \alpha_i-\frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_iy_j \langle x_i,x_j\rangle<br>\end{equation}$$</p>
<p>即 $\max_{\alpha_i\geq 0,r_i\geq 0} \min_{w,b,\xi}\mathcal{L}(w,b,\xi,\alpha,r)$ 简化为：<br>$$\begin{equation}\begin{array}{*{20}{l}}<br>&amp;\max \sum_{i=1}^n\alpha_i – \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle x_i,x_j\rangle\\<br>s.t., &amp; 0 \leq \alpha_i \color{red}{\leq C}, i=1,\ldots,n\\<br>&amp;\sum_{i=1}^n\alpha_iy_i = 0<br>\end{array}\end{equation}$$<br>可见，目标函数不变，只有 $\alpha_i$ 多了一个上限 $C$。而 Kernel 化的非线性形式也是一样的，只要把 $\langle x_i,x_j \rangle$ 换成 $\kappa(x_i,x_j)$ 即可。这样一来，一个完整的，可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机才终于介绍完毕了。 </p>
<hr>
<h2 id="5-主要参考"><a href="#5-主要参考" class="headerlink" title="5 主要参考"></a>5 主要参考</h2><p><a href="http://blog.csdn.net/v\_july\_v/article/details/7624837" target="_blank" rel="external">july-支持向量机通俗导论（理解SVM的三层境界）</a><br><a href="http://blog.pluskid.org/?page\_id=683" target="_blank" rel="external">pluskid-支持向量机系列</a></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/SVM/">SVM</a>
  </div>

</div>



</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/maven/" title="Maven 和 M2Eclipse配置">
  <span>
  Maven 和 M2Eclipse配置</span>
</a>
</div>


<div class="next">
<a href="/ml_note/"  title="机器学习的可行性">
 <span>机器学习的可行性
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="svm/" data-title="支持向量机SVM" data-url="http://www.stackess.com/svm/"></div>
</section>


</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-线性分类"><span class="toc-number">1.</span> <span class="toc-text">1 线性分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-magrin"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 magrin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-优化问题"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 优化问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Lagrangian对偶"><span class="toc-number">2.</span> <span class="toc-text">2 Lagrangian对偶</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-非线性扩展"><span class="toc-number">3.</span> <span class="toc-text">3 非线性扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-映射"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-核函数"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-几个核函数"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 几个核函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-异常点"><span class="toc-number">4.</span> <span class="toc-text">4 异常点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-主要参考"><span class="toc-number">5.</span> <span class="toc-text">5 主要参考</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">
<div id="authorInfo">
	
		<div class="author-img"></div>		
	
	<div class="my-social-info" class="clearfix">
		
		<a href="http://weibo.com/5261272169" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/cstackess" target="_blank" class="icon-mygithub" title="github"></a>
		
		
		
		
		
		
		
		
		
		
		<a href="mailto:cstackess@126.com" target="_blank" class="icon-email" title="Email Me"></a>
		

	</div>
</div>

  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/人工智能/" title="人工智能">人工智能<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/其他/" title="其他">其他<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/前端开发/" title="前端开发">前端开发<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/后台开发/" title="后台开发">后台开发<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/工具配置/" title="工具配置">工具配置<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/机器学习/" title="机器学习">机器学习<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法/" title="算法">算法<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
  <div class="tagcloudlist">
    <p class="asidetitle">Tag Cloud</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/Bandwangon/" style="font-size: 10px;">Bandwangon</a> <a href="/tags/Cygwin/" style="font-size: 12.5px;">Cygwin</a> <a href="/tags/Java/" style="font-size: 20px;">Java</a> <a href="/tags/Javascript/" style="font-size: 10px;">Javascript</a> <a href="/tags/LaTeX/" style="font-size: 10px;">LaTeX</a> <a href="/tags/Leetcode/" style="font-size: 10px;">Leetcode</a> <a href="/tags/Linux/" style="font-size: 17.5px;">Linux</a> <a href="/tags/M2Eclipse/" style="font-size: 10px;">M2Eclipse</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/Maven/" style="font-size: 10px;">Maven</a> <a href="/tags/Node-js/" style="font-size: 15px;">Node.js</a> <a href="/tags/PhpStorm/" style="font-size: 10px;">PhpStorm</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/ShadowSocks/" style="font-size: 10px;">ShadowSocks</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Socket-IO/" style="font-size: 10px;">Socket.IO</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WinSplit/" style="font-size: 10px;">WinSplit</a> <a href="/tags/awk/" style="font-size: 12.5px;">awk</a> <a href="/tags/gnuplot/" style="font-size: 10px;">gnuplot</a> <a href="/tags/grep/" style="font-size: 10px;">grep</a> <a href="/tags/log4j/" style="font-size: 10px;">log4j</a> <a href="/tags/npm/" style="font-size: 10px;">npm</a> <a href="/tags/五子棋/" style="font-size: 10px;">五子棋</a> <a href="/tags/信息论/" style="font-size: 10px;">信息论</a> <a href="/tags/大数据/" style="font-size: 12.5px;">大数据</a> <a href="/tags/开源/" style="font-size: 15px;">开源</a> <a href="/tags/弹幕/" style="font-size: 10px;">弹幕</a> <a href="/tags/微信公众平台/" style="font-size: 10px;">微信公众平台</a> <a href="/tags/微信支付/" style="font-size: 10px;">微信支付</a> <a href="/tags/机器学习基石/" style="font-size: 10px;">机器学习基石</a> <a href="/tags/树/" style="font-size: 10px;">树</a> <a href="/tags/源码分析/" style="font-size: 12.5px;">源码分析</a> <a href="/tags/熵/" style="font-size: 10px;">熵</a> <a href="/tags/爱好/" style="font-size: 10px;">爱好</a> <a href="/tags/算法导论/" style="font-size: 10px;">算法导论</a> <a href="/tags/闭包/" style="font-size: 10px;">闭包</a>
    </div>
  </div>


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>



</aside>
</div>
    </div>
    <footer><div id="footer" >
		
		
				<div class="cc-license">
          <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
            <img src="/img/cc-by-nc-sa.svg" alt="Creative Commons" />
          </a>
        </div>
   		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/hsihohuang/kiddochan" target="_blank" title="Kiddochan">Kiddochan</a> © 2016 
		
		<!--https://github.com/cstackess-->
		<a href="/about" target="_blank" title="cstackess">cstackess</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"stackess"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 









<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
